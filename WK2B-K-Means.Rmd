---
title: "R Notebook"
output: html_notebook
---


## Define the question
Identifing the different characteristics of customer groups. 

## The metric for success
Successfully identifing the customers who are most likely to click on Ad

## The context
Kira Plastinina is a Russian brand that is sold through a defunct chain of retail stores in Russia, Ukraine, Kazakhstan, Belarus, China, Philippines, and Armenia. The brand’s Sales and Marketing team would like to understand their customer’s behavior from data that they have collected over the past year. More specifically, they would like to learn the characteristics of customer groups.


## Experimental design taken
1. Problem Definition

2. Data Sourcing

3. Check the Data

4. Perform Data Cleaning

5. Perform Exploratory Data Analysis  (Univariate, Bivariate & Multivariate)

6. Implement the Solution

7. Challenge the Solution

8. Follow up Questions


## The appropriateness of the available data 
The data provided is sufficient to carry out our analysis.


## Importing the libraries we need
```{r}
library("Amelia")
library("mice")

library(dplyr)
library(tidyr)
library(corrplot)
library(ggplot2)
library("factoextra")
library("cluster")
library(gridExtra)

```



## Loading the dataset.
```{r}
econ_df = read.csv("http://bit.ly/EcommerceCustomersDataset")
View(econ_df)

```

## Checking the data
```{r}
# Checking for the first 6 rows
head(econ_df)

```


```{r}
# Checking for the first 6 rows
tail(econ_df)

```


```{r}
# Viewing the structure of the dataset
# ---
#
str(econ_df)

```
 We have 12330 rows and 18 columns


```{r}
# Checking whether each column has an appropriate datatype

sapply(econ_df, class)

```


```{r}
# Checking the unique values in the each of the columns

sapply(econ_df, unique)

```


## Tidying the Dataset
```{r}
# Changing the male and Clicked.on.Ad datatype
# Changing to factor
econ_df$Weekend = as.factor(as.numeric(econ_df$Weekend))
econ_df$Revenue = as.factor(as.numeric(econ_df$Revenue))
econ_df$VisitorType = as.integer(econ_df$VisitorType)

# Checkingclass changed
class(econ_df$Weekend)
class(econ_df$Revenue)
class(econ_df$VisitorType)


# The class changed from integer to factor

print("Summary after")
summary(econ_df)
```


```{r}
# Checking for missing values
# to calculate the number of na values
print("the number of na values")
sum(is.na(econ_df))

```
We have 112 missing values in our dataset


### MICE imputations
```{r}
# Removing the missing values

my_imp = mice(econ_df, m=5, method=c("mean", "pmm", "mean", "pmm", "cart", "pmm", "midastouch", "pmm", "",  "",  "",  "",  "",  "",  "",  "",  "", ""), maxit=20)


print("Check the summary of the ProductRelated column")
summary(econ_df$ProductRelated)


# For the indexes of the missing values, @ value of imp get the means
# check for the imp with the closest mean to bmi ie 26.56
# that is imp 4


my_imp$imp$ProductRelated

# For the clean record set
econ_final = complete(my_imp,1)

# Checking for the number of missing values
sapply(econ_final, function(x) sum(is.na(x)))

```



```{r}
# Confirming the missing values were removed
sum(is.na(econ_final))
```



```{r}
# Checking for duplicates

cat("The number of duplicate values are: ",sum(duplicated(econ_final)))

# Removing the duplicates
econ_final <- econ_final[!duplicated(econ_final), ]             # Remove duplicated rows

cat("\n")

# Checking to see if the duplicates were dropped
cat("The number of duplicate values to confirm they were dropped : ",sum(duplicated(econ_final)))

```
Duplicates successfully removed


### Dealing with outliers

```{r}
# To check for outliers
boxplot(econ_final)

```


```{r}
# We pick the specific columns that we want remove the outliers from
# To have a clear view of the outliers we remove some columns

check <- econ_final[, 1:7]
boxplot(check)


```


```{r}
# Summary to identify the 1st and 3rd quatile
summary(econ_final)

```


```{r}

# We do a summary for all the columns with outliers
# For Administrative Duration

IQR_admin_dur <- 93.50 - 0.00                  # Quantile Range
upfen_admin_dur = 0.00 + 1.5*IQR_admin_dur
upfen_admin_dur

# For Informational Duration

IQR_info_dur <- 0.00  - 0.00                  # Quantile Range
upfen_info_dur = 0.00  + 1.5*IQR_info_dur
upfen_info_dur


# For ProductRelated_Duration

IQR_prod_dur <- 1477.2  - 193.8                  # Quantile Range
upfen_prod_dur = 193.8   + 1.5*IQR_prod_dur
upfen_prod_dur


# The rest have have been kept because they are not many, therefore the outliers in those may not skew the analysis


econ_final = subset(econ_final, Administrative<=27.00  & Administrative_Duration<=140.25 & Informational<=2549.38 & Informational_Duration<=0 & ProductRelated<=705.00 & ProductRelated_Duration<2118.9 & BounceRates<=0.20000)

boxplot(econ_final)

```

The extreme outliers were removed



## Exploratory Data Analysis

### Univariate Analysis
```{r}
# Getting the central measures 
# ----

# Getting the mode
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

check2 <- econ_final[, 1:10]
summary(check2)

desc_stats <- data.frame(
  Min = apply(check2, 2, min),    # minimum
  Med = apply(check2, 2, median), # median
  Mean = apply(check2, 2, mean),  # mean
  SD = apply(check2, 2, sd),      # Standard deviation
  Max = apply(check2, 2, max),    # Maximum
  Var = apply(check2, 2, var),    # Variance
  Mode = apply(check2, 2, getmode)
  )

desc_stats <- round(desc_stats, 1)
head(desc_stats)

```


```{r}
# Histograms


econ_final%>% 
  gather(attributes, value, 1:10) %>%
  ggplot(aes(x = value)) +
  geom_histogram(fill = 'lightblue2', color = 'black') +
  facet_wrap(~attributes, scales = 'free_x') +
  labs(x="Values", y="Frequency") +
  theme_bw()


```

For the first 10 variables, most of the records taken lie within 0, even for ExitRates and ProductRelated as we can see below

```{r}
# Plotting the histograms

hist(econ_final$OperatingSystems, main="operating systems", xlab="Operating Systems", ylab="frequency")

hist(econ_final$Browser, main="Browser", xlab="browser", ylab="frequency")

hist(econ_final$Region, main="Region", xlab="region", ylab="frequency")

hist(econ_final$TrafficType, main="Traffic Type", xlab="traffic type", ylab="frequency")

hist(econ_final$ExitRates, main="Exit Rates", xlab="Exit Rates", ylab="frequency")

hist(econ_final$ProductRelated, main="Product Related", xlab="Product Related", ylab="frequency")
```

For the columns, most records were taken between;
1) For the operating system,  1.5 - 2
2) For browser, 1 - 2
3) For region, 1 - 2 as well
4) For traffic type, its also between 1 - 2
5) For exit rate, it is 0.00 - 0.002
6) For Product related, 0 - 10

```{r}
# Fetching the Administrative column
# ---
# 
month <- econ_final$Month

# Applying the table() function to compute the frequency distribution of the age variable
# ---
# 
month_frequency <- table(month)

# Printing age_frequency below
# ---
#

month_frequency

# Then applying the barplot function to produce its bar graph
# ---
# 
barplot(month_frequency)

```

The month that recorded the most numbers is May


```{r}
# The revenue distribution
reven <- econ_final$Revenue
revs <- table(reven)
barplot(revs, xlab = "Revenue")

```

For Revenue, False(0) recorded the most numbers


```{r}
# The weekend distribution
weekend <- econ_final$Weekend
week <- table(weekend)
barplot(week, xlab = "Weekend")

```

For weekend, False also recorded the most numbers 


```{r}
# The Visitor Type distribution
vis_type <- econ_final$VisitorType
vis <- table(vis_type)
barplot(vis, xlab = "VisitorType")

```

The visitor type that recorded 1the most numbers is returning visitor


### Bivariate Analysis

```{r}

# plot Product Realated vs. Product Related Duration 

ggplot(econ_final, 
       aes(x =ProductRelated_Duration, 
           y = ProductRelated )) +
  geom_point() + 
  labs(title = "Product Related vs. Product Related Duration")
```
As the product related duration increased the more product related pages 
It have a direct proportional relationship


```{r}
# checking correlation matrix
num_ads <- unlist(lapply(econ_final, is.numeric))
num_ad <- econ_final[ , num_ads]

corrplot(cor(num_ad), type = 'lower', method = 'number', tl.cex = 0.8, number.cex = 1.0)
```

### Multivariate Analysis
```{r}
# plot Product Related vs. Product Related Duration 
# (color represents Revenue)
ggplot(econ_final, 
       aes(x = ProductRelated_Duration, 
           y = ProductRelated, 
           color = Revenue)) +
  geom_point(size = 3, 
             alpha = .6) +
  labs(title = "Product Related vs. Product Related Duration") 

ggplot(econ_final, 
       aes(x = Administrative_Duration, 
           y = Administrative, 
           color = Revenue)) +
  geom_point(size = 3, 
             alpha = .6) +
  labs(title = "Administrative vs. Administrative_Duration") 

```
For both graphs, the red is dominating than the green indicating that false was the value that was most recorded when it came to both pages.


## Implementing Solution - Modeling
### Creating train/test
```{r}
X <- econ_final[, c(1, 2, 3, 4,5,6,7,8,9,10,12,13,14,15)]
y <- econ_final[, "Revenue"]

head(X)

```

```{r}

# Normalizing the dataset so that no particular attribute 
# has more impact on clustering algorithm than others.
# ---
# 

X_Norm <- as.data.frame(scale(X))
head(X_Norm)
```


```{r}
# Applying the K-means clustering algorithm with no. of centroids(k)=3
# ---
# 
result<- kmeans(X,3) 

# Previewing the no. of records in each cluster
# 
result$size 
```

```{r}
# Getting the value of cluster center datapoint value(3 centers for k=3)
# ---
# 
result$centers 
```


```{r}
# Getting the cluster vector that shows the cluster where each record falls
# ---
# 
result$cluster

```


```{r}
# Computing k-means clustering in R 
set.seed(123)

X_K2 <- kmeans(X_Norm, centers = 3, nstart = 25)
print(X_K2)

```


```{r}
#visualize the cluster  so far
#
fviz_cluster(X_K2, data = X_Norm)
```

```{r}
# Clusters to which each point is associated
X_K2$cluster
```

```{r}
# Cluster centers
X_K2$centers
```

```{r}
# Between clusters sum of square
X_K2$betweenss
```

```{r}
# Within cluster sum of square
X_K2$withinss
```

```{r}
# Total with sum of square
X_K2$tot.withinss

```


```{r}
# Total sum of square
X_K2$totss
```



```{r}
# We can execute the same process for 2, 3, 4, and 5 clusters, and the results are shown in the figure:

X_K3 <- kmeans(X_Norm, centers = 2, nstart = 25)
X_K4 <- kmeans(X_Norm, centers = 4, nstart = 25)
X_K5 <- kmeans(X_Norm, centers = 5, nstart = 25)
X_K6 <- kmeans(X_Norm, centers = 6, nstart = 25)

```


```{r}
#We can plot these clusters for different K value to compare.

p1 <- fviz_cluster(X_K3, geom = "point", data = X_Norm) + ggtitle(" K = 2")
p2 <- fviz_cluster(X_K4, geom = "point", data = X_Norm) + ggtitle(" K = 4")
p3 <- fviz_cluster(X_K5, geom = "point", data = X_Norm) + ggtitle(" K = 5")
p4 <- fviz_cluster(X_K6, geom = "point", data = X_Norm) + ggtitle(" K = 6")

grid.arrange(p1, p2, p3, p4, nrow = 2)
```
K = 2 is giving us a much better plot


## Challenging the Solution
### Determining Optimal Clusters

```{r}
# Determining Optimal clusters (k) Using 
# 1) Elbow method
fviz_nbclust(x = X_Norm,FUNcluster = kmeans, method = 'wss' )

```


```{r}
  wssplot <- function(data, nc = 15, set.seed = 1234){
    wss <- (nrow(data) - 1)*sum(apply(data, 2, var))
    for(i in 2:nc) {
      set.seed(1234)
      wss[i] <- sum(kmeans(x = data, centers = i, nstart = 25)$withinss)
    }
    plot(1:nc, wss, type = 'b', xlab = 'Number of Clusters', ylab = 'Within Group Sum of Square',
         main = 'Elbow Method Plot to Find Optimal Number of Clusters', frame.plot = T,
         col = 'blue', lwd = 1.5)
  }
  
  wssplot(X_Norm)
```


```{r}
# Determining Optimal clusters (k) Using 
# 2) Average Silhouette Method

fviz_nbclust(x = X_Norm,FUNcluster = kmeans, method = 'silhouette' )
```

```{r}
# Determining Optimal clusters (k) Using 
# 3) Gap Statistic


# gap_stat <- clusGap(x = irisNorm, FUN = kmeans, K.max = 15, nstart = 25, B = 50 )

# plot the result to determine the optimal number of clusters.
# fviz_gap_stat(gap_stat)

```

The gat statistic did not work therefore i opted to go with the first two
The K means did not do a good job with clustering this data


## Conclusions
1) For the first 10 variables, most of the records taken lie within 0, even for ExitRates and ProductRelated as we can see below

For the columns, most records were taken between;
2) For the operating system,  1.5 - 2
3) For browser, 1 - 2
4) For region, 1 - 2 as well
5) For traffic type, its also between 1 - 2
6) For exit rate, it is 0.00 - 0.002
7) For Product related, 0 - 10

8) The month that recorded the most numbers is May
9) For weekend, False also recorded the most numbers 
10) For Revenue, False(0) recorded the most numbers
11) The visitor type that recorded 1the most numbers is returning visitor
12) As the product related duration increased the more product related pages. It have a direct proportional relationship
13) For both graphs, the red is dominating than the green indicating that false was the value that was most recorded when it came to both pages. 















